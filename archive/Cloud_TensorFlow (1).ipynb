{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iJ7SbH-d1nC8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-25 16:35:51.462315: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-25 16:35:51.462362: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import PIL\n",
    "import shutil\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SoO12pRo4zDU",
    "outputId": "59e366e6-ddb2-4c27-fc62-2b901e944dbe"
   },
   "outputs": [],
   "source": [
    "# Unzip data file\n",
    "import zipfile as zf\n",
    "# files = zf.ZipFile(\"Cloud_Data.zip\", 'r')\n",
    "# files.extractall('directory to extract')\n",
    "# files.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30mGwdnQh1BL"
   },
   "outputs": [],
   "source": [
    "all_contrails = ('/content/drive/My Drive/DS Portfolio Projects/Contrail_and_Cloud_Classifier/Contrail_Files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vyg8fhqJyZ2P"
   },
   "outputs": [],
   "source": [
    "# Sebastian setup the variables x and y here\n",
    "# x (n x d) = cloud + contrails data \n",
    "# y (n x 1) = binary label data [cloud, contrail]\n",
    "# Where cloud = 0  and contrail = 1\n",
    "\n",
    "#df.dataframeName = 'St_clouds.csv'\n",
    "#display(df.shape)\n",
    "#display(df.dtypes)\n",
    "#display(df.columns)\n",
    "#display(df['class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sy5T9Nuq7UNd"
   },
   "outputs": [],
   "source": [
    "# Ensure that all images are about the same resolution\n",
    "# Verify the color gambit of the images, they should be (blue, and white)\n",
    "# Remove photos that are too large or too small\n",
    "# Print out 10 random images from each dataset (cloud and contrail)\n",
    "# Ensure that images are from the ground looking up in the day without a fish eye effect.\n",
    "# Make sure there are pictures of empty skies as well, those count as clouds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k38FxQfJ47yl"
   },
   "outputs": [],
   "source": [
    "# Processing to balance the dataset \n",
    "# Transforming, flipping, and modifying images\n",
    "# Goal: Make the contrail data set as large as the cloud dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Htgj71PB8TaO"
   },
   "outputs": [],
   "source": [
    "# Validate that the balanced dataset fits well\n",
    "# Print out side by side of transformations and the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8m1trNr8wxu4"
   },
   "outputs": [],
   "source": [
    "# TensorFlow model below\n",
    "# Uses convolution, maxpooling, dropout, and flatten layers\n",
    "# Sets up dense CNN model with keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Layers working with image data\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (11, 11), activation = 'relu', use_bias=True, kernel_regularizer =tf.keras.regularizers.l2( l=0.01), input_shape=(84, 173, 1)))\n",
    "model.add(layers.MaxPooling2D((3, 3)))\n",
    "model.add(layers.Conv2D(128, (5, 5), activation = 'relu', use_bias=True, kernel_regularizer =tf.keras.regularizers.l2( l=0.01)))\n",
    "model.add(layers.MaxPooling2D((3, 3)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation = 'relu', use_bias=True, kernel_regularizer =tf.keras.regularizers.l2( l=0.01)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation = 'relu', use_bias=True, kernel_regularizer =tf.keras.regularizers.l2( l=0.01)))\n",
    "model.add(layers.MaxPooling2D((3, 3)))\n",
    "\n",
    "# Fully connected layer with collapsed feature space\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(len(df['classID'].value_counts()), activation = \"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZqmqsraz2vi"
   },
   "outputs": [],
   "source": [
    "# Set a compiler\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITTiU2mHz3uV"
   },
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XQ-cfQu1cMI"
   },
   "outputs": [],
   "source": [
    "# Standardizing the data\n",
    "MaxAbsScaler().fit(x)\n",
    "X = transformer.transform(x)\n",
    "# Setup the training and validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 53)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYalW_ia1Le-"
   },
   "outputs": [],
   "source": [
    "# Fit the data\n",
    "history = model.fit(X_train, y_train, epochs=10, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zA5eZxsoz8Tu"
   },
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "plt.plot(history.history['val_loss'], color='b', label=\"validation loss\")\n",
    "plt.title(\"Test Loss\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Cloud_TensorFlow",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
