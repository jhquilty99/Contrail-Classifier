{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Contrail Classifier\n",
    "The goal of this project is to train a machine learning model that can accurately classify images of the sky as containing contrails.\n",
    "To build the model, we have obtained cloud data from four sources:\n",
    "1. [Cirrus Cumulus Stratus Nimbus (CCSN) Database](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CADDPD), this \n",
    "2. [Singapore Data Swimcat](https://ieeexplore.ieee.org/abstract/document/7350833)\n",
    "3. [CLASA](https://github.com/CLASA/Contrail-Machine-Vision), [Official Website](https://clasa.github.io/) a proposed solution to the NASA Clouds vs Contrails challenge.\n",
    "4. [Google Cloud Project](https://arxiv.org/abs/2304.02122)\n",
    "\n",
    "Potential applications are noted below:\n",
    "\n",
    "Potential Applications\n",
    "* Climate Studies: Contrails can have a significant impact on the Earth's atmosphere and climate. They can reflect sunlight back into space, contributing to global cooling, but they can also trap heat within the Earth's atmosphere, contributing to global warming. Therefore, a machine learning model trained to detect and monitor contrails could provide important data for climate researchers.\n",
    "\n",
    "* Air Traffic Control: A model trained to identify contrails could be useful in tracking aircraft routes and densities, particularly in areas with less developed radar infrastructure.\n",
    "\n",
    "* Aerospace and Defense: This model could be used for aerospace and defense purposes. For instance, detecting contrails could help in tracking and identifying stealth, unauthorized, or unrecognized flights, which can be important in maintaining airspace security."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Script\n",
    "Preprocessing is a crucial step in the machine learning pipeline because the quality and quantity of the data that you feed into your model will directly determine how well it can learn. Here are some reasons how we could preprocess image data:\n",
    "* Labeling: These images are not all labeled, and images from different datasets. The purpose of labeling is to homogenize the data so that each image is labeled in the same manner.\n",
    "\n",
    "* Image Resizing: In real-world scenarios, images can come in different sizes and aspect ratios. However, many computer vision models (like Convolutional Neural Networks) require images to be of a uniform size. Therefore, images often need to be resized to fit the requirements of the model.\n",
    "\n",
    "* Normalization: Image pixel intensities can range from 0 to 255. Normalizing these pixel intensities to a smaller range, often between 0 and 1 or -1 and 1, can help the model learn more effectively. This is because smaller, centered values are easier for the model's weight initialization and optimization process. Scaling the pixel values of the images to a small range like 0-1 or -1 to 1 can help the model converge faster during training. The 'Rescaling' layer in TensorFlow can be used for this purpose.\n",
    "\n",
    "* Data Augmentation: Image datasets can be augmented by applying random transformations like rotation, scaling, translation, flip etc. This can help increase the amount of training data and make the model more robust to variations in the input data that it hasn't seen before. This can help the model generalize better to new data. TensorFlow provides tools for data augmentation in the 'tf.keras.layers.experimental.preprocessing' module.\n",
    "\n",
    "* Dealing with Color Channels: Some models might require grayscale images, while others might require color images. Depending on the model, you might need to convert images from color to grayscale, or vice versa. Depending on your data, you might find that transforming the color space of your images (from RGB to HSV, Lab, YUV, etc.) could improve your model's ability to detect features.\n",
    "\n",
    "* Feature Extraction: In some cases, it might be beneficial to manually extract features from the images, such as edges, corners, and other local features. These can be used as inputs to the machine learning model. For contrails detection, specific filters that are sensitive to the features of contrails could be used. This might require some research and experimentation.\n",
    "\n",
    "* Dimensionality Reduction: Images are high-dimensional data, and it may be beneficial to reduce their dimensionality. This can be done through techniques like Principal Component Analysis (PCA) or autoencoders, which can make the model more efficient without losing too much information.\n",
    "\n",
    "* Balancing Classes: If the numbers of contrail and no-contrail images are not roughly equal, the model might become biased towards the more common class. Solutions include oversampling the minority class, undersampling the majority class, or using a combination of both.\n",
    "\n",
    "These preprocessing steps help to make the image data more suitable for computer vision models and can lead to better performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(image_dir, dictionary):\n",
    "    # List to hold all image data\n",
    "    images = list()\n",
    "    # List to hold image classifications (1,0)\n",
    "    classes = list()\n",
    "    # List to hold folders\n",
    "    folders = list()\n",
    "    # Specify the common size to resize all images\n",
    "    common_size = (400, 400)\n",
    "    # Iterate over each image in the directory\n",
    "    for folder in os.listdir(image_dir):\n",
    "        # Only open files with the specified filetype extension (e.g., \".png\" or \".jpg\")\n",
    "        for filename in os.listdir(os.path.join(image_dir, folder)):\n",
    "            if filename.endswith('.jpg'):\n",
    "                # Open each image file\n",
    "                img_path = os.path.join(image_dir, folder, filename)\n",
    "                img = Image.open(img_path)\n",
    "                # Resize image to the common size\n",
    "                img = ImageOps.fit(img, common_size, Image.Resampling.LANCZOS)\n",
    "                # Append the image data to your list\n",
    "                images.append(img)\n",
    "                classes.append(dictionary[folder])\n",
    "                folders.append(image_dir)\n",
    "    # Now the 'images' list contains all the images in the image_dir as PIL Image objects, with the labels in the 'classes' list\n",
    "    return np.array([np.array(image) for image in images]), np.array(classes), np.array(folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_proportion(classes):\n",
    "    print(f'This array contains {round(np.mean(100*classes),2)}% contrails')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"../data/Roboflow\"\n",
    "robo_dictionary = {\n",
    "    'Contrail':1,\n",
    "    'No_contrail':0\n",
    "}\n",
    "images_robo, classes_robo, folder_robo = image_loader(image_dir, robo_dictionary)\n",
    "print(f'From Roboflow we extract {len(images_robo)} images from {len(np.unique(classes_robo))} distinct classes')\n",
    "print_class_proportion(classes_robo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"../data/CCSN_v2\"\n",
    "ccsn_dictionary = {\n",
    "    'Ct':1,\n",
    "    'Ac':0, 'Sc':0, 'Ns':0, 'Cu':0, 'Ci':0, 'Cc':0, 'Cb':0, 'As':0, 'Cs':0, 'St':0\n",
    "}\n",
    "images_ccsn, classes_ccsn, folder_ccsn = image_loader(image_dir, ccsn_dictionary)\n",
    "print(f'From CCSN we extract {len(images_ccsn)} images from {len(np.unique(classes_ccsn))} distinct classes')\n",
    "print_class_proportion(classes_ccsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"../data/CLASA\"\n",
    "clasa_dictionary = {\n",
    "    'Contrail':1,\n",
    "    'Cirrus':0\n",
    "}\n",
    "images_clasa, classes_clasa, folder_clasa = image_loader(image_dir, clasa_dictionary)\n",
    "print(f'From CLASA we extract {len(images_clasa)} images from {len(np.unique(classes_clasa))} distinct classes')\n",
    "print_class_proportion(classes_clasa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"../data/Singapore Data Swimcat\"\n",
    "singapore_dictionary = {\n",
    "    'A-sky':0,\n",
    "    'B-pattern':0,\n",
    "    'C-thick-dark':0,\n",
    "    'D-thick-white':0,\n",
    "    'E-veil':0\n",
    "}\n",
    "images_singapore, classes_singapore, folder_singapore = image_loader(image_dir, singapore_dictionary)\n",
    "print(f'From Singapore we extract {len(images_singapore)} images from {len(np.unique(classes_singapore))} distinct classes')\n",
    "print_class_proportion(classes_singapore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the folders into two lists: one containing images, and the other has the labels\n",
    "images_all = np.concatenate([images_robo, images_ccsn, images_clasa, images_singapore])\n",
    "classes_all = np.concatenate([classes_robo, classes_ccsn, classes_clasa, classes_singapore])\n",
    "folders_all = np.concatenate([folder_robo, folder_ccsn, folder_clasa, folder_singapore])\n",
    "print(images_all[0:5])\n",
    "print(classes_all[0:5])\n",
    "print(f'In total we have extracted {len(images_all)} images from {len(np.unique(classes_all))} distinct classes')\n",
    "# Do some feature analysis to see the distribution of response variable\n",
    "print_class_proportion(classes_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some feature analysis to see the distribution of input iamges\n",
    "def image_dimension(images):\n",
    "    # Get dimensions of images\n",
    "    dimensions = [img.shape for img in images]\n",
    "\n",
    "    # Split dimensions into two lists: width and height\n",
    "    widths = [dim[1] for dim in dimensions]\n",
    "    heights = [dim[0] for dim in dimensions]\n",
    "\n",
    "\n",
    "    # Create subplots: 2 rows, 1 column\n",
    "    fig = make_subplots(rows=3, cols=1)\n",
    "\n",
    "    # Add histogram for widths to the first subplot\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=widths, name='widths', opacity=0.75),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add histogram for heights to the second subplot\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=heights, name='heights', opacity=0.75),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Add histogram for heights to the second subplot\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=np.array(widths)/np.array(heights), name='aspect_ratio', opacity=0.75),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "    # Update xaxis titles\n",
    "    fig.update_xaxes(title_text='Widths', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Heights', row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Aspect Ratio', row=3, col=1)\n",
    "\n",
    "    # Update yaxis titles\n",
    "    fig.update_yaxes(title_text='Count', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Count', row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Count', row=3, col=1)\n",
    "\n",
    "    # Update layout to show subplots\n",
    "    fig.update_layout(\n",
    "        title_text='Distribution of Image Widths and Heights', # title of plot\n",
    "        height=600, # height of plot in pixels\n",
    "        width=900, # width of plot in pixels\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dimension(images_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most common pixel size so that we can make squares out of that size\n",
    "# Get dimensions of images\n",
    "dimensions = [img.shape for img in images_all]\n",
    "\n",
    "# Split dimensions into two lists: width and height\n",
    "# For numpy array dimensions, the first dimension is height and the second one is width\n",
    "heights = [dim[0] for dim in dimensions]\n",
    "widths = [dim[1] for dim in dimensions]\n",
    "\n",
    "most_common_width = Counter(widths).most_common(1)[0][0]\n",
    "most_common_height = Counter(heights).most_common(1)[0][0]\n",
    "print(f'From all images, the most_common width is {most_common_width} px and the most common height is {most_common_height} px')\n",
    "print(f'From all images, the smallest width is {np.min(widths)} px and the maximum width is {np.max(widths)} px')\n",
    "print(f'From all images, the smallest height is {np.min(heights)} px and the maximum width is {np.max(heights)} px')\n",
    "\n",
    "# Get the index of the biggest image in the dimensions list\n",
    "def biggest_image(dimensions):\n",
    "    for i in range(0, len(dimensions)): \n",
    "        # use dim[1] to check the width, because dim[0] is the height for numpy arrays\n",
    "        if dimensions[i][1] == np.max(widths):\n",
    "            return i\n",
    "\n",
    "print(f'From all images biggest image is {biggest_image(dimensions)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***COMMENTED OUT Does not work with PIL library, therefore we resized images above***\n",
    "# Make the images square and do other transformations\n",
    "\n",
    "#pixels = 400\n",
    "#images_all_squared = []\n",
    "#classes_all_squared = []\n",
    "#i = 0\n",
    "# this will not work because Pil objects use .shhape and not .size\n",
    "#for img in images_all:\n",
    "    #width, height = img.size\n",
    "    #if width < pixels or height < pixels:\n",
    "        # Calculate padding\n",
    "       # width_padding = pixels - width\n",
    "      #  height_padding = pixels - height\n",
    "        # Apply padding with a grey background\n",
    "     #   images_all_squared.append(ImageOps.pad(img, (pixels,pixels), color=125))\n",
    "    #elif width > 400 or height > 400:\n",
    "        # Try both shrinking the image and cropping the image to create syntetic samples\n",
    "        # Image.LANCZOS applies a high-quality downsampling filter\n",
    "       # images_all_squared.append(img.resize((pixels, pixels), Image.LANCZOS))\n",
    "        # Returns a resized and cropped version of the image, cropped to the requested aspect ratio and size.\n",
    "      #  images_all_squared.append(ImageOps.fit(img, (pixels, pixels)))\n",
    "        # Because we are expanding the dataset, make sure to add the extra class labels\n",
    "     #   classes_all_squared.append(classes_all[i])\n",
    "    #else:\n",
    "   #     images_all_squared.append(img)\n",
    "  #  classes_all_squared.append(classes_all[i])\n",
    " #   i += 1\n",
    "#\n",
    "#print(f'In total we have turned {len(images_all)} raw images into {len(images_all_squared)} square images with {pixels} px sides')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_dimension(images_all_squared)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This one uses the resized NP data\n",
    "# Assume X is your array of features and y are the labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_all, classes_all, test_size=0.2, random_state=42)\n",
    "print(f'X_train is {len(X_train)} images long')\n",
    "print(f'X_test is {len(X_test)} images long')\n",
    "print(f'y_train is {len(y_train)} labels long')\n",
    "print(f'y_test is {len(y_test)} labels long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***Commented out reasons outlined above***\n",
    "\n",
    "# Assume X is your array of features and y are the labels\n",
    "#X_train, X_test, y_train, y_test = train_test_split(images_all_squared, classes_all_squared, test_size=0.2, random_state=42)\n",
    "#print(f'X_train is {len(X_train)} images long')\n",
    "#print(f'X_test is {len(X_test)} images long')\n",
    "#print(f'y_train is {len(y_train)} labels long')\n",
    "#print(f'y_test is {len(y_test)} labels long')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Color Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_datagen = ImageDataGenerator(\n",
    "#     rescale = 1/255,\n",
    "#     shear_range = 0.2,\n",
    "#     zoom_range = 0.2,\n",
    "#     horizontal_flip = True\n",
    "#     )\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "#random_datagen.fit(np.array(X_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pre-built ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# Add a logistic layer for binary classification\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# First: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Model in Azure Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "\n",
    "subscription_id = '81ccb9cf-adc5-4eac-b781-5b1fc76926f7'\n",
    "resource_group = 'ContrailResourceGroup'\n",
    "workspace = 'Contrail-Classifier'\n",
    "\n",
    "tenant_id = \"85b77843-1ccd-4a9b-9b12-baa93c8b4d37\"\n",
    "\n",
    "# Connect to the workspace\n",
    "ml_client = MLClient(InteractiveBrowserCredential(tenant_id=tenant_id), subscription_id, resource_group, workspace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a compute resource for training\n",
    "\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# Specify aml compute name\n",
    "cpu_compute_target = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    ml_client.compute.get(cpu_compute_target)\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "    compute = AmlCompute(\n",
    "        name=cpu_compute_target, size=\"STANDARD_D2s_v3\", min_instances=0, max_instances=4\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(compute).result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Environment, Experiment, ScriptRunConfig\n",
    "\n",
    "# Connect to the workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Creates a new Azure Machine Learning Environment\n",
    "myenv = Environment(\"myenv\")\n",
    "\n",
    "# Enable Docker and add necessary packages\n",
    "myenv.docker.enabled = True\n",
    "myenv.python.conda_dependencies.add_pip_package(\"keras\")\n",
    "myenv.python.conda_dependencies.add_pip_package(\"numpy\")\n",
    "myenv.python.conda_dependencies.add_pip_package(\"tensorflow\")\n",
    "myenv.python.conda_dependencies.add_pip_package(\"scikit-learn\")\n",
    "myenv.python.conda_dependencies.add_pip_package(\"azure-ai-ml\")\n",
    "myenv.python.conda_dependencies.add_pip_package(\"Pillow\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ScriptRunConfig\n",
    "\n",
    "source_directory = 'C:/Users/sebas/OneDrive/Documents/Contrail/Contrail-Classifier/code'  \n",
    "script = 'Ground_Based_Training_script.py'\n",
    "compute_target = \"cpu-cluster\"\n",
    "\n",
    "# Create the ScriptRunConfig object\n",
    "src = ScriptRunConfig(source_directory=source_directory, script=script, compute_target=compute_target, environment=myenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: contrail_classifier_1690952204_c78a77a3\n",
      "Web View: https://ml.azure.com/runs/contrail_classifier_1690952204_c78a77a3?wsid=/subscriptions/81ccb9cf-adc5-4eac-b781-5b1fc76926f7/resourcegroups/ContrailResourceGroup/workspaces/Contrail-Classifier&tid=85b77843-1ccd-4a9b-9b12-baa93c8b4d37\n",
      "\n",
      "Streaming azureml-logs/20_image_build_log.txt\n",
      "=============================================\n",
      "\n",
      "2023/08/02 04:56:52 Downloading source code...\n",
      "2023/08/02 04:56:53 Finished downloading source code\n",
      "2023/08/02 04:56:54 Creating Docker network: acb_default_network, driver: 'bridge'\n",
      "2023/08/02 04:56:54 Successfully set up Docker network: acb_default_network\n",
      "2023/08/02 04:56:54 Setting up Docker configuration...\n",
      "2023/08/02 04:56:55 Successfully set up Docker configuration\n",
      "2023/08/02 04:56:55 Logging in to registry: 19fdcc1f9a94406985698a8d8e694586.azurecr.io\n",
      "2023/08/02 04:56:55 Successfully logged into 19fdcc1f9a94406985698a8d8e694586.azurecr.io\n",
      "2023/08/02 04:56:56 Volume source scriptsFromEms successfully created\n",
      "2023/08/02 04:56:56 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2023/08/02 04:56:56 Scanning for dependencies...\n",
      "2023/08/02 04:56:57 Successfully scanned dependencies\n",
      "2023/08/02 04:56:57 Launching container with name: acb_step_0\n",
      "Sending build context to Docker daemon  77.31kB\n",
      "\n",
      "Step 1/21 : FROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20230620.v1@sha256:d87e60a40761fd1bb9efa97d94a9ec52b62a9e7081fd2acd5c7d94ba2852d60a\n",
      "mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20230620.v1@sha256:d87e60a40761fd1bb9efa97d94a9ec52b62a9e7081fd2acd5c7d94ba2852d60a: Pulling from azureml/openmpi4.1.0-ubuntu20.04\n",
      "f0412dfb1aae: Already exists\n",
      "2335b08db55d: Pulling fs layer\n",
      "4c6821e3fe1d: Pulling fs layer\n",
      "5932a200f0bf: Pulling fs layer\n",
      "33c1fe43af46: Pulling fs layer\n",
      "e97cedf7a795: Pulling fs layer\n",
      "b2f0588cc721: Pulling fs layer\n",
      "fcf20f972570: Pulling fs layer\n",
      "b727183c795c: Pulling fs layer\n",
      "87ffdc8de2d2: Pulling fs layer\n",
      "33c1fe43af46: Waiting\n",
      "e97cedf7a795: Waiting\n",
      "b2f0588cc721: Waiting\n",
      "fcf20f972570: Waiting\n",
      "87ffdc8de2d2: Waiting\n",
      "4c6821e3fe1d: Verifying Checksum\n",
      "4c6821e3fe1d: Download complete\n",
      "5932a200f0bf: Verifying Checksum\n",
      "5932a200f0bf: Download complete\n",
      "33c1fe43af46: Verifying Checksum\n",
      "33c1fe43af46: Download complete\n",
      "b2f0588cc721: Verifying Checksum\n",
      "b2f0588cc721: Download complete\n",
      "e97cedf7a795: Verifying Checksum\n",
      "e97cedf7a795: Download complete\n",
      "fcf20f972570: Verifying Checksum\n",
      "fcf20f972570: Download complete\n",
      "b727183c795c: Verifying Checksum\n",
      "b727183c795c: Download complete\n",
      "87ffdc8de2d2: Verifying Checksum\n",
      "87ffdc8de2d2: Download complete\n",
      "2335b08db55d: Verifying Checksum\n",
      "2335b08db55d: Download complete\n",
      "2335b08db55d: Pull complete\n",
      "4c6821e3fe1d: Pull complete\n",
      "5932a200f0bf: Pull complete\n",
      "33c1fe43af46: Pull complete\n",
      "e97cedf7a795: Pull complete\n",
      "b2f0588cc721: Pull complete\n",
      "fcf20f972570: Pull complete\n",
      "b727183c795c: Pull complete\n",
      "87ffdc8de2d2: Pull complete\n",
      "Digest: sha256:d87e60a40761fd1bb9efa97d94a9ec52b62a9e7081fd2acd5c7d94ba2852d60a\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20230620.v1@sha256:d87e60a40761fd1bb9efa97d94a9ec52b62a9e7081fd2acd5c7d94ba2852d60a\n",
      " ---> 59f39c44073e\n",
      "Step 2/21 : USER root\n",
      " ---> Running in 344833db6e59\n",
      "Removing intermediate container 344833db6e59\n",
      " ---> dc756fb64a5b\n",
      "Step 3/21 : RUN mkdir -p $HOME/.cache\n",
      " ---> Running in c97db8d58f7d\n",
      "Removing intermediate container c97db8d58f7d\n",
      " ---> 1227abf9ede2\n",
      "Step 4/21 : WORKDIR /\n",
      " ---> Running in ca639bfbb9e9\n",
      "Removing intermediate container ca639bfbb9e9\n",
      " ---> 4764f580b37e\n",
      "Step 5/21 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\n",
      " ---> 6b94c5e2ec7e\n",
      "Step 6/21 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\n",
      " ---> Running in f9e4c0a5e6c6\n",
      "Removing intermediate container f9e4c0a5e6c6\n",
      " ---> 51948bcc07d2\n",
      "Step 7/21 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\n",
      " ---> dfa53d1bab5c\n",
      "Step 8/21 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_25db23ddda1880ffc26d032761a7488f -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \"$HOME/.cache/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\n",
      " ---> Running in cec2a905dbaa\n",
      "Retrieving notices: ...working... done\n",
      "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
      "Collecting package metadata (repodata.json): ...working... \n"
     ]
    }
   ],
   "source": [
    "# Create an experiment\n",
    "experiment_name = 'contrail_classifier'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "# Submit the experiment\n",
    "run = experiment.submit(config=src)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "When dealing with imbalanced classes, traditional metrics like accuracy can be misleading. For a task where avoiding false positives (i.e., the model predicting a positive class when it's actually negative) is important, you might want to consider the following metrics:\n",
    "\n",
    "* Precision: Precision is the ratio of true positives (TP) to the sum of true positives and false positives (FP). Precision is directly concerned with minimizing false positive predictions. Precision = TP / (TP + FP)\n",
    "\n",
    "* F1 Score: The F1 score is the harmonic mean of precision and recall. While it doesn't directly focus on false positives, it provides a balance between precision and recall. This can be useful if both false positives and false negatives are of concern.\n",
    "\n",
    "* Area Under the Precision-Recall Curve (AUPRC): In an imbalanced classification problem, AUPRC can be a better metric than traditional ones. It calculates the area under the curve formed by plotting recall (x-axis) against precision (y-axis) at various threshold settings. The closer this area is to 1, the better the model is at distinguishing between the positive and negative classes.\n",
    "\n",
    "For the cost function in the training phase of a neural network, the standard is cross-entropy loss. When dealing with imbalanced classes, one way to handle this is by applying class weights to the loss function, which assigns a higher penalty for misclassifying the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
