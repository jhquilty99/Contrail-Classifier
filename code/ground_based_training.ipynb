{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Contrail Classifier\n",
    "The goal of this project is to train a machine learning model that can accurately classify images of the sky as containing contrails.\n",
    "To build the model, we have obtained cloud data from four sources:\n",
    "1. [Cirrus Cumulus Stratus Nimbus (CCSN) Database](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CADDPD), this \n",
    "2. [Singapore Data Swimcat](https://ieeexplore.ieee.org/abstract/document/7350833)\n",
    "3. [CLASA](https://github.com/CLASA/Contrail-Machine-Vision), [Official Website](https://clasa.github.io/) a proposed solution to the NASA Clouds vs Contrails challenge.\n",
    "4. [Google Cloud Project](https://arxiv.org/abs/2304.02122)\n",
    "\n",
    "Potential applications are noted below:\n",
    "\n",
    "Potential Applications\n",
    "* Climate Studies: Contrails can have a significant impact on the Earth's atmosphere and climate. They can reflect sunlight back into space, contributing to global cooling, but they can also trap heat within the Earth's atmosphere, contributing to global warming. Therefore, a machine learning model trained to detect and monitor contrails could provide important data for climate researchers.\n",
    "\n",
    "* Air Traffic Control: A model trained to identify contrails could be useful in tracking aircraft routes and densities, particularly in areas with less developed radar infrastructure.\n",
    "\n",
    "* Aerospace and Defense: This model could be used for aerospace and defense purposes. For instance, detecting contrails could help in tracking and identifying stealth, unauthorized, or unrecognized flights, which can be important in maintaining airspace security."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Script\n",
    "Preprocessing is a crucial step in the machine learning pipeline because the quality and quantity of the data that you feed into your model will directly determine how well it can learn. Here are some reasons how we could preprocess image data:\n",
    "* Labeling: These images are not all labeled, and images from different datasets. The purpose of labeling is to homogenize the data so that each image is labeled in the same manner.\n",
    "\n",
    "* Image Resizing: In real-world scenarios, images can come in different sizes and aspect ratios. However, many computer vision models (like Convolutional Neural Networks) require images to be of a uniform size. Therefore, images often need to be resized to fit the requirements of the model.\n",
    "\n",
    "* Normalization: Image pixel intensities can range from 0 to 255. Normalizing these pixel intensities to a smaller range, often between 0 and 1 or -1 and 1, can help the model learn more effectively. This is because smaller, centered values are easier for the model's weight initialization and optimization process. Scaling the pixel values of the images to a small range like 0-1 or -1 to 1 can help the model converge faster during training. The 'Rescaling' layer in TensorFlow can be used for this purpose.\n",
    "\n",
    "* Data Augmentation: Image datasets can be augmented by applying random transformations like rotation, scaling, translation, flip etc. This can help increase the amount of training data and make the model more robust to variations in the input data that it hasn't seen before. This can help the model generalize better to new data. TensorFlow provides tools for data augmentation in the 'tf.keras.layers.experimental.preprocessing' module.\n",
    "\n",
    "* Dealing with Color Channels: Some models might require grayscale images, while others might require color images. Depending on the model, you might need to convert images from color to grayscale, or vice versa. Depending on your data, you might find that transforming the color space of your images (from RGB to HSV, Lab, YUV, etc.) could improve your model's ability to detect features.\n",
    "\n",
    "* Feature Extraction: In some cases, it might be beneficial to manually extract features from the images, such as edges, corners, and other local features. These can be used as inputs to the machine learning model. For contrails detection, specific filters that are sensitive to the features of contrails could be used. This might require some research and experimentation.\n",
    "\n",
    "* Dimensionality Reduction: Images are high-dimensional data, and it may be beneficial to reduce their dimensionality. This can be done through techniques like Principal Component Analysis (PCA) or autoencoders, which can make the model more efficient without losing too much information.\n",
    "\n",
    "* Balancing Classes: If the numbers of contrail and no-contrail images are not roughly equal, the model might become biased towards the more common class. Solutions include oversampling the minority class, undersampling the majority class, or using a combination of both.\n",
    "\n",
    "These preprocessing steps help to make the image data more suitable for computer vision models and can lead to better performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(image_dir, dictionary):\n",
    "    # List to hold all image data\n",
    "    images = list()\n",
    "    # List to hold image classifications (1,0)\n",
    "    classes = list()\n",
    "    # List to hold folders\n",
    "    folders = list()\n",
    "    # Specify the common size to resize all images\n",
    "    common_size = (400, 400)\n",
    "    # Iterate over each image in the directory\n",
    "    for folder in os.listdir(image_dir):\n",
    "        # Only open files with the specified filetype extension (e.g., \".png\" or \".jpg\")\n",
    "        for filename in os.listdir(os.path.join(image_dir, folder)):\n",
    "            if filename.endswith('.jpg'):\n",
    "                # Open each image file\n",
    "                img_path = os.path.join(image_dir, folder, filename)\n",
    "                img = Image.open(img_path)\n",
    "                # Resize image to the common size\n",
    "                img = ImageOps.fit(img, common_size, Image.Resampling.LANCZOS)\n",
    "                # Append the image data to your list\n",
    "                images.append(img)\n",
    "                classes.append(dictionary[folder])\n",
    "                folders.append(image_dir)\n",
    "    # Now the 'images' list contains all the images in the image_dir as PIL Image objects, with the labels in the 'classes' list\n",
    "    return np.array([np.array(image) for image in images]), np.array(classes), np.array(folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_proportion(classes):\n",
    "    print(f'This array contains {round(np.mean(100*classes),2)}% contrails')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From CLASA we extract 694 images from 2 distinct classes\n",
      "This array contains 54.9% contrails\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"../data/Roboflow\"\n",
    "robo_dictionary = {\n",
    "    'Contrail':1,\n",
    "    'No_contrail':0\n",
    "}\n",
    "images_robo, classes_robo, folder_robo = image_loader(image_dir, robo_dictionary)\n",
    "print(f'From Roboflow we extract {len(images_robo)} images from {len(np.unique(classes_robo))} distinct classes')\n",
    "print_class_proportion(classes_robo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From CCSN we extract 2543 images from 2 distinct classes\n",
      "This array contains 7.86% contrails\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"../data/CCSN_v2\"\n",
    "ccsn_dictionary = {\n",
    "    'Ct':1,\n",
    "    'Ac':0, 'Sc':0, 'Ns':0, 'Cu':0, 'Ci':0, 'Cc':0, 'Cb':0, 'As':0, 'Cs':0, 'St':0\n",
    "}\n",
    "images_ccsn, classes_ccsn, folder_ccsn = image_loader(image_dir, ccsn_dictionary)\n",
    "print(f'From CCSN we extract {len(images_ccsn)} images from {len(np.unique(classes_ccsn))} distinct classes')\n",
    "print_class_proportion(classes_ccsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From CLASA we extract 454 images from 2 distinct classes\n",
      "This array contains 76.87% contrails\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"../data/CLASA\"\n",
    "clasa_dictionary = {\n",
    "    'Contrail':1,\n",
    "    'Cirrus':0\n",
    "}\n",
    "images_clasa, classes_clasa, folder_clasa = image_loader(image_dir, clasa_dictionary)\n",
    "print(f'From CLASA we extract {len(images_clasa)} images from {len(np.unique(classes_clasa))} distinct classes')\n",
    "print_class_proportion(classes_clasa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Singapore we extract 782 images from 1 distinct classes\n",
      "This array contains 0.0% contrails\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"../data/Singapore Data Swimcat\"\n",
    "singapore_dictionary = {\n",
    "    'A-sky':0,\n",
    "    'B-pattern':0,\n",
    "    'C-thick-dark':0,\n",
    "    'D-thick-white':0,\n",
    "    'E-veil':0\n",
    "}\n",
    "images_singapore, classes_singapore, folder_singapore = image_loader(image_dir, singapore_dictionary)\n",
    "print(f'From Singapore we extract {len(images_singapore)} images from {len(np.unique(classes_singapore))} distinct classes')\n",
    "print_class_proportion(classes_singapore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 23  57 103]\n",
      "   [ 25  59 105]\n",
      "   [ 24  58 104]\n",
      "   ...\n",
      "   [108 125 153]\n",
      "   [108 125 153]\n",
      "   [108 125 153]]\n",
      "\n",
      "  [[ 22  56 102]\n",
      "   [ 25  59 105]\n",
      "   [ 24  58 104]\n",
      "   ...\n",
      "   [110 127 155]\n",
      "   [110 127 155]\n",
      "   [110 127 155]]\n",
      "\n",
      "  [[ 22  56 102]\n",
      "   [ 24  58 104]\n",
      "   [ 24  58 104]\n",
      "   ...\n",
      "   [111 128 156]\n",
      "   [111 128 156]\n",
      "   [111 128 156]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0   0  11]\n",
      "   [  0   1  12]\n",
      "   [  4  10  21]\n",
      "   ...\n",
      "   [103 126 150]\n",
      "   [103 126 150]\n",
      "   [103 126 150]]\n",
      "\n",
      "  [[  1   2   9]\n",
      "   [  1   4  11]\n",
      "   [  0   1   8]\n",
      "   ...\n",
      "   [ 95 123 153]\n",
      "   [ 96 123 153]\n",
      "   [ 96 123 153]]\n",
      "\n",
      "  [[  2   0   5]\n",
      "   [  1   0   4]\n",
      "   [  1   0   4]\n",
      "   ...\n",
      "   [ 91 121 155]\n",
      "   [ 93 122 155]\n",
      "   [ 93 122 155]]]\n",
      "\n",
      "\n",
      " [[[  6  10  69]\n",
      "   [ 10  27 111]\n",
      "   [  0  23 131]\n",
      "   ...\n",
      "   [  1  12  67]\n",
      "   [  2  13  69]\n",
      "   [  2  13  69]]\n",
      "\n",
      "  [[  6  11  69]\n",
      "   [ 11  29 113]\n",
      "   [  0  26 133]\n",
      "   ...\n",
      "   [  1  12  67]\n",
      "   [  2  13  69]\n",
      "   [  2  13  69]]\n",
      "\n",
      "  [[  6  11  69]\n",
      "   [ 13  31 115]\n",
      "   [  2  29 136]\n",
      "   ...\n",
      "   [  1  12  67]\n",
      "   [  2  13  69]\n",
      "   [  2  13  69]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 93  94  96]\n",
      "   [255 255 255]\n",
      "   [237 238 240]\n",
      "   ...\n",
      "   [ 21  65 118]\n",
      "   [ 27  71 124]\n",
      "   [ 31  76 128]]\n",
      "\n",
      "  [[ 93  94  96]\n",
      "   [255 255 255]\n",
      "   [237 238 240]\n",
      "   ...\n",
      "   [  9  50 104]\n",
      "   [ 14  56 110]\n",
      "   [ 20  62 116]]\n",
      "\n",
      "  [[ 93  94  96]\n",
      "   [255 255 255]\n",
      "   [237 238 240]\n",
      "   ...\n",
      "   [ 12  52 107]\n",
      "   [ 10  51 105]\n",
      "   [ 10  50 105]]]\n",
      "\n",
      "\n",
      " [[[  1  12  68]\n",
      "   [  2  13  69]\n",
      "   [  3  14  70]\n",
      "   ...\n",
      "   [ 33  74 128]\n",
      "   [ 22  60 115]\n",
      "   [ 11  48 103]]\n",
      "\n",
      "  [[  1  12  68]\n",
      "   [  2  13  69]\n",
      "   [  2  13  69]\n",
      "   ...\n",
      "   [ 30  71 125]\n",
      "   [ 16  55 110]\n",
      "   [ 13  50 105]]\n",
      "\n",
      "  [[  0  11  66]\n",
      "   [  2  12  68]\n",
      "   [  2  13  68]\n",
      "   ...\n",
      "   [ 27  67 122]\n",
      "   [ 12  50 105]\n",
      "   [ 15  52 107]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0  31 132]\n",
      "   [  0  32 132]\n",
      "   [  0  32 132]\n",
      "   ...\n",
      "   [242 243 245]\n",
      "   [242 243 245]\n",
      "   [242 243 245]]\n",
      "\n",
      "  [[  0  29 131]\n",
      "   [  0  28 130]\n",
      "   [  0  30 131]\n",
      "   ...\n",
      "   [241 242 244]\n",
      "   [241 242 244]\n",
      "   [241 242 244]]\n",
      "\n",
      "  [[  0  28 130]\n",
      "   [  0  26 128]\n",
      "   [  0  27 129]\n",
      "   ...\n",
      "   [241 242 244]\n",
      "   [241 242 244]\n",
      "   [241 242 244]]]\n",
      "\n",
      "\n",
      " [[[ 10  41  85]\n",
      "   [ 10  41  85]\n",
      "   [  9  40  84]\n",
      "   ...\n",
      "   [114 143 174]\n",
      "   [107 136 168]\n",
      "   [ 94 119 155]]\n",
      "\n",
      "  [[ 10  41  85]\n",
      "   [ 10  41  85]\n",
      "   [  9  40  84]\n",
      "   ...\n",
      "   [115 144 175]\n",
      "   [104 132 166]\n",
      "   [ 97 123 158]]\n",
      "\n",
      "  [[ 10  41  85]\n",
      "   [ 10  41  85]\n",
      "   [  9  40  84]\n",
      "   ...\n",
      "   [113 142 173]\n",
      "   [ 98 126 159]\n",
      "   [ 98 124 159]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  1  12  28]\n",
      "   [  0   7  24]\n",
      "   [  0   2  19]\n",
      "   ...\n",
      "   [119 161 192]\n",
      "   [120 162 193]\n",
      "   [121 163 194]]\n",
      "\n",
      "  [[  3   0   3]\n",
      "   [  1   0   4]\n",
      "   [  1   0   5]\n",
      "   ...\n",
      "   [115 159 191]\n",
      "   [116 159 191]\n",
      "   [117 160 192]]\n",
      "\n",
      "  [[ 10   0   0]\n",
      "   [ 11   0   0]\n",
      "   [  7   1   0]\n",
      "   ...\n",
      "   [113 156 188]\n",
      "   [113 156 188]\n",
      "   [114 157 189]]]\n",
      "\n",
      "\n",
      " [[[ 12  56 143]\n",
      "   [ 13  57 144]\n",
      "   [ 15  59 146]\n",
      "   ...\n",
      "   [253 255 254]\n",
      "   [253 255 254]\n",
      "   [253 255 254]]\n",
      "\n",
      "  [[ 13  57 144]\n",
      "   [ 13  57 144]\n",
      "   [ 14  58 145]\n",
      "   ...\n",
      "   [253 255 254]\n",
      "   [253 255 254]\n",
      "   [253 255 254]]\n",
      "\n",
      "  [[ 13  57 144]\n",
      "   [ 13  57 144]\n",
      "   [ 13  57 144]\n",
      "   ...\n",
      "   [253 255 254]\n",
      "   [253 255 254]\n",
      "   [253 255 254]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  2   2   2]\n",
      "   [  2   2   2]\n",
      "   [  2   2   2]\n",
      "   ...\n",
      "   [ 85 140 180]\n",
      "   [ 84 137 179]\n",
      "   [ 83 135 177]]\n",
      "\n",
      "  [[  1   1   1]\n",
      "   [  1   1   1]\n",
      "   [  1   1   1]\n",
      "   ...\n",
      "   [ 81 135 174]\n",
      "   [ 81 134 174]\n",
      "   [ 79 132 172]]\n",
      "\n",
      "  [[  1   1   1]\n",
      "   [  1   1   1]\n",
      "   [  1   1   1]\n",
      "   ...\n",
      "   [ 81 136 173]\n",
      "   [ 81 135 173]\n",
      "   [ 80 134 172]]]]\n",
      "[1 1 1 1 1]\n",
      "In total we have extracted 4473 images from 2 distinct classes\n",
      "This array contains 20.79% contrails\n"
     ]
    }
   ],
   "source": [
    "# Merge all the folders into two lists: one containing images, and the other has the labels\n",
    "images_all = np.concatenate([images_robo, images_ccsn, images_clasa, images_singapore])\n",
    "classes_all = np.concatenate([classes_robo, classes_ccsn, classes_clasa, classes_singapore])\n",
    "folders_all = np.concatenate([folder_robo, folder_ccsn, folder_clasa, folder_singapore])\n",
    "print(images_all[0:5])\n",
    "print(classes_all[0:5])\n",
    "print(f'In total we have extracted {len(images_all)} images from {len(np.unique(classes_all))} distinct classes')\n",
    "# Do some feature analysis to see the distribution of response variable\n",
    "print_class_proportion(classes_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some feature analysis to see the distribution of input iamges\n",
    "def image_dimension(images):\n",
    "    # Get dimensions of images\n",
    "    dimensions = [img.size for img in images]\n",
    "\n",
    "    # Split dimensions into two lists: width and height\n",
    "    #### CHANGED \"dim\" to \"dimensions\"\n",
    "    widths = [dimensions[0] for dim in dimensions]\n",
    "    heights = [dimensions[1] for dim in dimensions]\n",
    "\n",
    "    # Create subplots: 2 rows, 1 column\n",
    "    fig = make_subplots(rows=3, cols=1)\n",
    "\n",
    "    # Add histogram for widths to the first subplot\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=widths, name='widths', opacity=0.75),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add histogram for heights to the second subplot\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=heights, name='heights', opacity=0.75),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Add histogram for heights to the second subplot\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=np.array(widths)/np.array(heights), name='aspect_ratio', opacity=0.75),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "    # Update xaxis titles\n",
    "    fig.update_xaxes(title_text='Widths', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Heights', row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Aspect Ratio', row=3, col=1)\n",
    "\n",
    "    # Update yaxis titles\n",
    "    fig.update_yaxes(title_text='Count', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Count', row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Count', row=3, col=1)\n",
    "\n",
    "    # Update layout to show subplots\n",
    "    fig.update_layout(\n",
    "        title_text='Distribution of Image Widths and Heights', # title of plot\n",
    "        height=600, # height of plot in pixels\n",
    "        width=900, # width of plot in pixels\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image_dimension(images_all)\n",
      "Cell \u001b[1;32mIn[9], line 49\u001b[0m, in \u001b[0;36mimage_dimension\u001b[1;34m(images)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39m# Update layout to show subplots\u001b[39;00m\n\u001b[0;32m     43\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(\n\u001b[0;32m     44\u001b[0m     title_text\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDistribution of Image Widths and Heights\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m# title of plot\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     height\u001b[39m=\u001b[39m\u001b[39m600\u001b[39m, \u001b[39m# height of plot in pixels\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     width\u001b[39m=\u001b[39m\u001b[39m900\u001b[39m, \u001b[39m# width of plot in pixels\u001b[39;00m\n\u001b[0;32m     47\u001b[0m )\n\u001b[1;32m---> 49\u001b[0m fig\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\plotly\\basedatatypes.py:3409\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3376\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3377\u001b[0m \u001b[39mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[0;32m   3378\u001b[0m \u001b[39mspecified by the renderer argument\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3405\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3406\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3407\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[1;32m-> 3409\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39;49mshow(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\plotly\\io\\_renderers.py:396\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    393\u001b[0m         )\n\u001b[0;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nbformat \u001b[39mor\u001b[39;00m Version(nbformat\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m Version(\u001b[39m\"\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 396\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    397\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    398\u001b[0m         )\n\u001b[0;32m    400\u001b[0m     ipython_display\u001b[39m.\u001b[39mdisplay(bundle, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    402\u001b[0m \u001b[39m# external renderers\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "image_dimension(images_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m dimensions \u001b[39m=\u001b[39m [img\u001b[39m.\u001b[39msize \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images_all]\n\u001b[0;32m      5\u001b[0m \u001b[39m# Split dimensions into two lists: width and height\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m widths \u001b[39m=\u001b[39m [dim[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m dim \u001b[39min\u001b[39;49;00m dimensions]\n\u001b[0;32m      7\u001b[0m heights \u001b[39m=\u001b[39m [dim[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m dimensions]\n\u001b[0;32m      8\u001b[0m most_common_width \u001b[39m=\u001b[39m Counter(widths)\u001b[39m.\u001b[39mmost_common(\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m dimensions \u001b[39m=\u001b[39m [img\u001b[39m.\u001b[39msize \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images_all]\n\u001b[0;32m      5\u001b[0m \u001b[39m# Split dimensions into two lists: width and height\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m widths \u001b[39m=\u001b[39m [dim[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m dimensions]\n\u001b[0;32m      7\u001b[0m heights \u001b[39m=\u001b[39m [dim[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m dimensions]\n\u001b[0;32m      8\u001b[0m most_common_width \u001b[39m=\u001b[39m Counter(widths)\u001b[39m.\u001b[39mmost_common(\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Find the most common pixel size so that we can make squares out of that size\n",
    "# Get dimensions of images\n",
    "dimensions = [img.size for img in images_all]\n",
    "\n",
    "# Split dimensions into two lists: width and height\n",
    "widths = [dim[0] for dim in dimensions]\n",
    "heights = [dim[1] for dim in dimensions]\n",
    "most_common_width = Counter(widths).most_common(1)[0][0]\n",
    "most_common_height = Counter(heights).most_common(1)[0][0]\n",
    "print(f'From all images, the most_common width is {most_common_width} px and the most common height is {most_common_height} px')\n",
    "print(f'From all images, the smallest width is {np.min(widths)} px and the maximum width is {np.max(widths)} px')\n",
    "print(f'From all images, the smallest height is {np.min(heights)} px and the maximum width is {np.max(heights)} px')\n",
    "\n",
    "# Get the index of the biggest image in the dimensions list\n",
    "def biggest_image(dimensions):\n",
    "    for i in range(0, len(dimensions)): \n",
    "       if dimensions[i][0] == np.max(widths):\n",
    "            return i\n",
    "    \n",
    "\n",
    "print(f'From all images biggest image is {biggest_image(dimensions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images_all:\n\u001b[1;32m----> 7\u001b[0m     width, height \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39msize\n\u001b[0;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m width \u001b[39m<\u001b[39m pixels \u001b[39mor\u001b[39;00m height \u001b[39m<\u001b[39m pixels:\n\u001b[0;32m      9\u001b[0m         \u001b[39m# Calculate padding\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         width_padding \u001b[39m=\u001b[39m pixels \u001b[39m-\u001b[39m width\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "# Make the images square and do other transformations\n",
    "pixels = 400\n",
    "images_all_squared = []\n",
    "classes_all_squared = []\n",
    "i = 0\n",
    "for img in images_all:\n",
    "    width, height = img.size\n",
    "    if width < pixels or height < pixels:\n",
    "        # Calculate padding\n",
    "        width_padding = pixels - width\n",
    "        height_padding = pixels - height\n",
    "        # Apply padding with a grey background\n",
    "        images_all_squared.append(ImageOps.pad(img, (pixels,pixels), color=125))\n",
    "    elif width > 400 or height > 400:\n",
    "        # Try both shrinking the image and cropping the image to create syntetic samples\n",
    "        # Image.LANCZOS applies a high-quality downsampling filter\n",
    "        images_all_squared.append(img.resize((pixels, pixels), Image.LANCZOS))\n",
    "        # Returns a resized and cropped version of the image, cropped to the requested aspect ratio and size.\n",
    "        images_all_squared.append(ImageOps.fit(img, (pixels, pixels)))\n",
    "        # Because we are expanding the dataset, make sure to add the extra class labels\n",
    "        classes_all_squared.append(classes_all[i])\n",
    "    else:\n",
    "        images_all_squared.append(img)\n",
    "    classes_all_squared.append(classes_all[i])\n",
    "    i += 1\n",
    "\n",
    "print(f'In total we have turned {len(images_all)} raw images into {len(images_all_squared)} square images with {pixels} px sides')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image_dimension(images_all_squared)\n",
      "Cell \u001b[1;32mIn[12], line 48\u001b[0m, in \u001b[0;36mimage_dimension\u001b[1;34m(images)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39m# Update layout to show subplots\u001b[39;00m\n\u001b[0;32m     42\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(\n\u001b[0;32m     43\u001b[0m     title_text\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDistribution of Image Widths and Heights\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m# title of plot\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     height\u001b[39m=\u001b[39m\u001b[39m600\u001b[39m, \u001b[39m# height of plot in pixels\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     width\u001b[39m=\u001b[39m\u001b[39m900\u001b[39m, \u001b[39m# width of plot in pixels\u001b[39;00m\n\u001b[0;32m     46\u001b[0m )\n\u001b[1;32m---> 48\u001b[0m fig\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\plotly\\basedatatypes.py:3409\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3376\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3377\u001b[0m \u001b[39mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[0;32m   3378\u001b[0m \u001b[39mspecified by the renderer argument\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3405\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3406\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3407\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[1;32m-> 3409\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39;49mshow(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\plotly\\io\\_renderers.py:396\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    393\u001b[0m         )\n\u001b[0;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nbformat \u001b[39mor\u001b[39;00m Version(nbformat\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m Version(\u001b[39m\"\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 396\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    397\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    398\u001b[0m         )\n\u001b[0;32m    400\u001b[0m     ipython_display\u001b[39m.\u001b[39mdisplay(bundle, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    402\u001b[0m \u001b[39m# external renderers\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "image_dimension(images_all_squared)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[39m# Assume X is your array of features and y are the labels\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(images_all_squared, classes_all_squared, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mX_train is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(X_train)\u001b[39m}\u001b[39;00m\u001b[39m images long\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mX_test is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(X_test)\u001b[39m}\u001b[39;00m\u001b[39m images long\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[0;32m   2564\u001b[0m )\n\u001b[0;32m   2566\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume X is your array of features and y are the labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_all_squared, classes_all_squared, test_size=0.2, random_state=42)\n",
    "print(f'X_train is {len(X_train)} images long')\n",
    "print(f'X_test is {len(X_test)} images long')\n",
    "print(f'y_train is {len(y_train)} labels long')\n",
    "print(f'y_test is {len(y_test)} labels long')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Color Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_datagen = ImageDataGenerator(\n",
    "#     rescale = 1/255,\n",
    "#     shear_range = 0.2,\n",
    "#     zoom_range = 0.2,\n",
    "#     horizontal_flip = True\n",
    "#     )\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "#random_datagen.fit(np.array(X_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pre-built ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# Add a logistic layer for binary classification\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# First: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#empty_X = np.empty(len(X_train), dtype=object)\n",
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "\n",
    "X_train_tensor = tf.convert_to_tensor([np.array(img) for img in X_train])\n",
    "y_train_tensor = tf.convert_to_tensor([np.array(img) for img in y_train])\n",
    "X_test_tensor = tf.convert_to_tensor([np.array(img) for img in X_test])\n",
    "y_test_tensor = tf.convert_to_tensor([np.array(img) for img in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight = 'balanced',\n",
    "    classes = np.unique(y_train),\n",
    "    y = y_train\n",
    ")\n",
    "\n",
    "# Convert class_weights to a dictionary to include it in model.fit()\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on batches with real-time data augmentation:\n",
    "# Pass class_weights to the fit function\n",
    "# model.fit(random_datagen.flow(X_train, y_train, batch_size=32, subset='training'),\n",
    "#          validation_data=random_datagen.flow(X_train, y_train, batch_size=8, subset='validation'),\n",
    "#          steps_per_epoch=len(X_train) / 32, epochs=32, class_weights = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'PIL.JpegImagePlugin.JpegImageFile'>\", \"<class 'PIL.Image.Image'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int32'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      2\u001b[0m     x \u001b[39m=\u001b[39;49m X_train,\n\u001b[0;32m      3\u001b[0m     y \u001b[39m=\u001b[39;49m y_train,\n\u001b[0;32m      4\u001b[0m     \u001b[39m# validation_data = (X_test, y_test),\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m     \u001b[39m# steps_per_epoch= len(X_train) / 32, \u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m     \u001b[39m# epochs=32, \u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m     \u001b[39m# class_weight = class_weights\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jhqui\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\jhqui\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1082\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1079\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1080\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1081\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1082\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1083\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1084\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1085\u001b[0m         )\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'PIL.JpegImagePlugin.JpegImageFile'>\", \"<class 'PIL.Image.Image'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int32'>\"})"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x = X_train_tensor,\n",
    "    y = y_train_tensor,\n",
    "    # validation_data = (X_test, y_test),\n",
    "    # steps_per_epoch= len(X_train) / 32, \n",
    "    # epochs=32, \n",
    "    # class_weight = class_weights\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "When dealing with imbalanced classes, traditional metrics like accuracy can be misleading. For a task where avoiding false positives (i.e., the model predicting a positive class when it's actually negative) is important, you might want to consider the following metrics:\n",
    "\n",
    "* Precision: Precision is the ratio of true positives (TP) to the sum of true positives and false positives (FP). Precision is directly concerned with minimizing false positive predictions. Precision = TP / (TP + FP)\n",
    "\n",
    "* F1 Score: The F1 score is the harmonic mean of precision and recall. While it doesn't directly focus on false positives, it provides a balance between precision and recall. This can be useful if both false positives and false negatives are of concern.\n",
    "\n",
    "* Area Under the Precision-Recall Curve (AUPRC): In an imbalanced classification problem, AUPRC can be a better metric than traditional ones. It calculates the area under the curve formed by plotting recall (x-axis) against precision (y-axis) at various threshold settings. The closer this area is to 1, the better the model is at distinguishing between the positive and negative classes.\n",
    "\n",
    "For the cost function in the training phase of a neural network, the standard is cross-entropy loss. When dealing with imbalanced classes, one way to handle this is by applying class weights to the loss function, which assigns a higher penalty for misclassifying the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
